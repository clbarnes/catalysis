{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, starting from a lineage-labeled neuron, we collect lineages nearby and check to see if the annotation is indeed correct, based on a naive Bayes classifier trained on the local environment around a particular lineage entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import catalysis as cat\n",
    "import catalysis.pynblast as pynblast\n",
    "import catalysis.plt as catplt\n",
    "import catalysis.transform as transform\n",
    "import catalysis.completeness as completeness\n",
    "import catalysis.lineage_classifier as lineage_classifier\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "import dill as pickle\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from itertools import chain, cycle\n",
    "from sklearn import cluster\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "HERE = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1data = cat.CatmaidDataInterface.from_json(os.environ[\"CATMAID_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_fly_f = pd.read_csv(os.path.join(HERE, \"data/smat_jefferis.csv\"), delimiter=\" \")\n",
    "smat = pynblast.ScoreMatrixLookup.from_dataframe(adult_fly_f)\n",
    "# Do this to reduce things to L1 volume size and change scale to nm, based on scale up observed in the L1/L3 data papers and desired properties\n",
    "smat.d_range = smat.d_range * 1000 / 4\n",
    "\n",
    "with open(os.path.join(HERE, \"data/Brain_Lineage_Landmarks_EMtoEM_ProjectSpace.csv\") as fid:\n",
    "    lin_landmarks = pd.read_csv(fid)\n",
    "\n",
    "side_name = [\"l_loc\", \"r_loc\"]\n",
    "\n",
    "wb_match = re.compile(\"\\*.*akira\")\n",
    "lineage_parser = re.compile(\"\\*(?P<group>.*?)_(?P<instance>[rl]) akira\")\n",
    "hemilateral_groups = l1data.match_groups_from_select_annotations(\n",
    "    wb_match, lineage_parser\n",
    ")\n",
    "\n",
    "all_lins = []\n",
    "for lin in sorted(list(hemilateral_groups.keys())):\n",
    "    for side in hemilateral_groups[lin]:\n",
    "        all_lins.append(hemilateral_groups[lin][side])\n",
    "\n",
    "lin_df = lineage_classifier.lineage_table(hemilateral_groups, lin_landmarks, side_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Continued thoughts on bundling approach.\n",
    "\n",
    "We want to use NBLAST to identify what lineage neurons should belong to. While in principle, lineage bundles are fairly clear, in practice that is not the case at all. In some instances, you really need the EM to guide you into seeing who genuinely bundles with whom. Part of this may be that the NBLAST cost function needs to be re-jiggered for this purpose. Bundled neurites are vastly more similar than is the case with whole arbors in transformed spaces, after all. The first step really should be to re-compute the NBLAST scores in this particular domain. However, there will still be outliers, and the clustering should be softer than a simple k-means would properly give you, giving you a ranked list of candidates.\n",
    "\n",
    "For a given neuron $\\textbf{x}$ and lineage $L_i \\in \\{L_i\\}$, we want to find the lineage $i$ which maximizes $P( \\textbf{x} \\in L_i | <\\textbf{x},\\textbf{y}>_{\\textbf{y} \\in L_i})$, where $<\\textbf{x},\\textbf{y}>_{y \\ in L_i}$ indicates the mean NBLAST distance to all neurons in $L_i$. \n",
    "After trivial application of Bayes's rule, the probability of $\\textbf{x} \\in L_i$ given the evidence is:\n",
    "$$\n",
    "P( \\textbf{x} \\in L_i \\mid <\\textbf{x},\\textbf{y}>_{\\textbf{y} \\in L_i}) = \n",
    "    P\\left( <\\textbf{x},\\textbf{y}>_{\\textbf{y} \\in L_i} \\mid \\textbf{x} \\in L_i \\right)\n",
    "    \\frac{P\\left( \\textbf{x} \\in L_i \\right)}{P\\left( <\\textbf{x},\\textbf{y}>_{\\textbf{y} \\in L_i} \\right)}\n",
    "$$\n",
    "\n",
    "We need to do comparison for those lineages for a specific region determined by the neuron at hand, since some lineages have more similar lineages than others. This is effectively like hand-setting distant lineages to probability 0 and not bothering with the computations.\n",
    "\n",
    "By pre-computing the NBLAST distance scores, all of these values can be computed quickly, leaving only $N \\times 1$ NBLAST comparisons (where N is the number of proximate neurites, for which the dotprops themselves can be pre-computed). Retraining can happen on that matrix.\n",
    "\n",
    "* We can estimate $P( <\\mathbf{x},\\mathbf{y}>_{\\mathbf{y} \\in L_i} \\mid \\mathbf{x} \\in L_i )$ for each $i$ (using all $\\mathbf{x} in L_i$).\n",
    "* We can estimate $P( <\\mathbf{x},\\mathbf{y}>_{\\mathbf{y} \\in L_i} \\mid \\mathbf{x} \\notin L_i )$ for each $i$ (using all $\\mathbf{x} not in L_i$).\n",
    "* Assume $P(\\mathbf{x} \\in L_i )$ is flat and cancels out in ratio comparisions.\n",
    "\n",
    "Now compute the ratio of the likelihood that a neuron is in the lineage given the data to the likelihood that it is not.\n",
    "$\\fract{P( <\\mathbf{x},\\mathbf{y}>_{\\mathbf{y} \\in L_i} \\mid \\mathbf{x} \\in L_i )}{P( <\\mathbf{x},\\mathbf{y}>_{\\mathbf{y} \\in L_i} \\mid \\mathbf{x} \\noin L_i )} $ for all proximate lineages $i$.\n",
    "\n",
    "For each lineage annotation, we're going to go through each neuron and train the above on all other neurons. If the most likely lineage is still the top pick, then we're good and move on. If not, we double check by plotting the lineages or, potentially, going to the EM data directly to look at bundling properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_ind = 15\n",
    "side = \"l\"\n",
    "\n",
    "nearby_neuron_radius = 2500\n",
    "nearby_lineage_radius = 4 * nearby_neuron_radius\n",
    "min_cable = 5000\n",
    "l_span = 30 * 1000\n",
    "resample_distance = 1000\n",
    "reroot_skeletons = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lin = sorted(list(hemilateral_groups.keys()))[lin_ind]\n",
    "\n",
    "# Compute distances between our base location and other lineages.\n",
    "if side == \"l\":\n",
    "    xyz0 = hemilateral_groups[base_lin][\"l_loc\"]\n",
    "elif side == \"r\":\n",
    "    xyz0 = hemilateral_groups[base_lin][\"r_loc\"]\n",
    "\n",
    "ds = sp.spatial.distance.cdist(xyz0, np.array(list(lin_df.xyz.values)))[0]\n",
    "lin_df_sp = lin_df.assign(ds=ds)\n",
    "\n",
    "readable_side = {\"l\": \"left side\", \"r\": \"right side\"}\n",
    "print(\"Working on {}, {}\".format(base_lin, readable_side[side]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Find nearby lineages\n",
    "\n",
    "This uses 'nearby_lineage_radius' to find lineage entry points near the base neuron. Optionally, reroot skeletons based on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all annotation_ids corresponding to lineages\n",
    "\n",
    "all_lin_ids = l1data.parse_annotation_list(all_lins, output=\"ids\")\n",
    "\n",
    "# Find lineages that are within `nearby_neuron_radius` of the base point\n",
    "rel_lins = [\n",
    "    hemilateral_groups[row[1].lin][row[1].side[0]]\n",
    "    for row in lin_df_sp[lin_df_sp.ds < nearby_lineage_radius].iterrows()\n",
    "]\n",
    "base_lin_ids = l1data.get_ids_from_annotations(\n",
    "    hemilateral_groups[base_lin][side], flatten=True\n",
    ")\n",
    "near_lin_ids = l1data.get_ids_from_annotations(rel_lins, flatten=True)\n",
    "\n",
    "# Make sure that the skeletons have proper roots\n",
    "if reroot_skeletons:\n",
    "    l1data.reroot_neurons_to_soma(near_lin_ids)\n",
    "\n",
    "# Find which lineage annotations are associated with which proximate skeletons\n",
    "multi_anno_ids = []\n",
    "for skid in base_lin_ids:\n",
    "    sk_annos = set(all_lin_ids).intersection(\n",
    "        set(l1data.get_annotations_for_objects([skid]))\n",
    "    )\n",
    "    if len(sk_annos) > 1:\n",
    "        multi_anno_ids.append(skid)\n",
    "        print(\"{} has multiple lineage annotations!\".format(skid))\n",
    "        l1data.url_to_neurons(skid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Now we need to compute the base probabilities for the lineage groups in this particular region.\n",
    "*This could be fixed by pre-computing all lineage-lineage nblast scores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_lin_nrns = cat.NeuronList.from_id_list(near_lin_ids, l1data)\n",
    "print(\"The nearby lineages are: {}\".format(rel_lins))\n",
    "# Compute dotprops for NBLAST. Make one longer than the other so that targets of queries don't have an artificial truncation.\n",
    "initial_seg_dotprops_short = lineage_classifier.compute_initial_segment_dotprops(\n",
    "    near_lin_nrns, l_span, resample_distance\n",
    ")\n",
    "initial_seg_dotprops_long = lineage_classifier.compute_initial_segment_dotprops(\n",
    "    near_lin_nrns, 2 * l_span, resample_distance\n",
    ")\n",
    "\n",
    "# Get convenience interpreters.\n",
    "# Mapping from lineage to skids beloning to it. Remove problem ids with multiple annotations.\n",
    "lin_dict = l1data.get_ids_from_annotations(rel_lins)\n",
    "\n",
    "lin_name2id = {name: l1data.parse_annotation_list(name)[0] for name in rel_lins}\n",
    "lin_id2name = {lin_name2id[name]: name for name in lin_name2id}\n",
    "\n",
    "skid2lin = {}\n",
    "skid2lin_name = {}\n",
    "for lin in lin_dict:\n",
    "    for skid in lin_dict[lin]:\n",
    "        skid2lin[skid] = lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be pre-computed\n",
    "lineage_lineage_nblast = pynblast.nblast_neurons(\n",
    "    smat,\n",
    "    nrns_q=initial_seg_dotprops_short,\n",
    "    nrns_t=initial_seg_dotprops_long,\n",
    "    as_dotprop=True,\n",
    "    normalize=True,\n",
    ").pivot(index=\"Queries\", columns=\"Targets\", values=\"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute likelihood ratios for each neuron in the base lineage, trained on all other neurons in that lineage (i.e. leave one out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_ids = []\n",
    "problem_table = []\n",
    "for rel_skid in base_lin_ids:\n",
    "    P_match, P_unmatch = lineage_classifier.compute_conditional_distributions(\n",
    "        lineage_lineage_nblast, lin_dict, lin_name2id, rel_lins, skip_skids=rel_skid\n",
    "    )\n",
    "    kdes_match, kdes_match_norm = lineage_classifier.fit_gaussian_kdes(P_match)\n",
    "    kdes_unmatch, kdes_unmatch_norm = lineage_classifier.fit_gaussian_kdes(P_unmatch)\n",
    "    suggested_matches = lineage_classifier.lineage_likelihood_ratios(\n",
    "        rel_skid,\n",
    "        lineage_lineage_nblast,\n",
    "        lin_dict,\n",
    "        lin_name2id,\n",
    "        rel_lins,\n",
    "        kdes_match,\n",
    "        kdes_match_norm,\n",
    "        kdes_unmatch,\n",
    "        kdes_unmatch_norm,\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        suggested_matches[\n",
    "            suggested_matches.Lineage == hemilateral_groups[base_lin][side]\n",
    "        ].FractionOfBest.values\n",
    "        < 1\n",
    "    ):\n",
    "        problem_table.append(suggested_matches)\n",
    "        problem_ids.append(rel_skid)\n",
    "\n",
    "if len(problem_table) > 1:\n",
    "    problem_table = pd.concat(problem_table, ignore_index=True)\n",
    "    print(\"{} skeleton ids need to be checked\".format(len(problem_ids)))\n",
    "    focused_table = problem_table[\n",
    "        (problem_table.FractionOfBest == 1)\n",
    "        | (problem_table.Lineage == hemilateral_groups[base_lin][side])\n",
    "    ].reset_index(drop=True)\n",
    "elif len(problem_table) == 1:\n",
    "    problem_table = problem_table[0]\n",
    "    print(\"{} skeleton ids need to be checked\".format(len(problem_ids)))\n",
    "    focused_table = problem_table[\n",
    "        (problem_table.FractionOfBest == 1)\n",
    "        | (problem_table.Lineage == hemilateral_groups[base_lin][side])\n",
    "    ].reset_index(drop=True)\n",
    "else:\n",
    "    print(\"No skeleton ids need to be checked\")\n",
    "    focused_table = pd.DataFrame(\n",
    "        {\"FractionOfBest\": [], \"LikelihoodRatio\": [], \"Lineage\": [], \"SkeletonId\": []}\n",
    "    )\n",
    "focused_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the candidate matches to confirm suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in a row number from the above table, corresponding to the entry with the 1.00 in fraction of best (this will be even)\n",
    "row_num = 3\n",
    "\n",
    "rel_skid = focused_table[focused_table.index == row_num].SkeletonId.values[0]\n",
    "suggest_lin = focused_table[focused_table.index == row_num].Lineage.values[0]\n",
    "if rel_skid in multi_anno_ids:\n",
    "    display(HTML(\"<mark><b>NEURON HAS MULIPLE LINEAGE ANNOTATIONS!</b></mark>\"))\n",
    "l1data.url_to_neurons(rel_skid)\n",
    "display(HTML(\"<b>Current lineage:</b> {}\".format(hemilateral_groups[base_lin][side])))\n",
    "display(\n",
    "    HTML('<b><font color=\"blue\">Suggested lineage:</b> {}</font>'.format(suggest_lin))\n",
    ")\n",
    "\n",
    "\n",
    "data = []\n",
    "data.append(\n",
    "    catplt.path_data(\n",
    "        initial_seg_dotprops_long[rel_skid][:, 0:3],\n",
    "        color=(0.9, 0, 0.1),\n",
    "        width=5,\n",
    "        name=near_lin_nrns[rel_skid].name,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the suggested lineage\n",
    "if suggest_lin != hemilateral_groups[base_lin][side]:\n",
    "    for skid in lin_dict[lin_name2id[suggest_lin]]:\n",
    "        data.append(\n",
    "            catplt.path_data(\n",
    "                initial_seg_dotprops_long[skid][:, 0:3],\n",
    "                color=(0.1, 0.1, 0.9),\n",
    "                width=1,\n",
    "                name=suggest_lin,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Plot the currently assigned lineage\n",
    "for skid in lin_dict[lin_name2id[hemilateral_groups[base_lin][side]]]:\n",
    "    data.append(\n",
    "        catplt.path_data(\n",
    "            initial_seg_dotprops_long[skid][:, 0:3],\n",
    "            color=(0.5, 0.5, 0.5),\n",
    "            width=1,\n",
    "            name=hemilateral_groups[base_lin][side],\n",
    "        )\n",
    "    )\n",
    "\n",
    "layout = go.Layout({\"showlegend\": False, \"width\": 800, \"height\": 800})\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example code below to plot all the local environment lineages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clrs = plt.get_cmap(\"Set1\").colors\n",
    "color_cycle = cycle(plt.get_cmap(\"Dark2\").colors)\n",
    "\n",
    "clr_dict = {}\n",
    "\n",
    "for lb in np.unique(np.unique(list(lin_dict.keys()))):\n",
    "    clr_dict[lb] = next(color_cycle)\n",
    "\n",
    "data = []\n",
    "for skid in initial_seg_dotprops:\n",
    "    lb = skid2lin[skid]\n",
    "    data.append(\n",
    "        catplt.path_data(\n",
    "            initial_seg_dotprops[skid][:, 0:3],\n",
    "            color=clr_dict[lb],\n",
    "            width=4,\n",
    "            name=lin_id2name[lb],\n",
    "        )\n",
    "    )\n",
    "\n",
    "layout = go.Layout({\"showlegend\": False, \"width\": 500, \"height\": 500})\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit ('catalysis37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4383f31ee38a20a76cbec2b62c3b7a23890c34105422706a1c1bcc5feaad3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
